\chapter{Introduction}
\pagenumbering{arabic}

%世界中のコンピュータサイエンスカリキュラムにデータ構造とアルゴリズムに関するコースが含まれている。データ構造は重要だ。生活の質を上げ、日常生活を効率的にしてくれる。数百万ドル、数十億ドルの企業にはデータ構造を中心に作られたものも多い。
データ構造とアルゴリズムに関するコースは世界の全てのコンピュータサイエンスカリキュラムに含まれている。データ構造はそれほどに重要だ。生活の質を上げるだけでなく、毎日のように人の命さえ救っている。データ構造によって数百万ドル、数十億ドルの規模にまでなった企業も多い。

% どういうものなのだろう？深く考えるのをやめると、データ構造と普段から接していることに気づく。
% How can this be (important)? では?
なぜデータ構造はこんなにも重要なのだろう？立ち止まって考えてみると、私達は普段からデータ構造と接している。

\begin{itemize}
	\item ファイルを開く：ファイルシステム\index{file system}のデータ構造を使って、ファイルをディスク上に配置し、検索できる。これは簡単ではない。ディスクには数億ものブロックがある。ファイルの内容はそのどこかに保存されるのだ。
	\item 連絡先を検索する：ダイヤル途中の部分的な情報にもとづき、連絡先リスト\index{contact list}から電話番号を見つけるためにデータ構造が使われる。これは簡単ではない。連絡先リストに含まれる情報はとても多いかもしれない。これまで電話や電子メールで連絡したことのある全ての人を想像してみてほしい。また、電話のプロセッサはあまり高性能でなく、メモリも潤沢でない。
	\item SNSにログインする：\index{social network}ネットワークサーバーは、ログイン情報からアカウント情報を検索する。これは簡単ではない。人気のソーシャルネットワークには何億人ものアクティブユーザーがいる。
	\item Webページを検索する：\index{web search}検索エンジンは検索語を含むWebページを見つけるためにデータ構造を使う。これは簡単ではない。インターネットには85億以上のWebページがあり、個々のページには検索されるかもしれない単語が多く含まれている。
	\item 緊急サービス（9-1-1）に電話する：\index{emergency services}\index{9-1-1}緊急サービスネットワークはパトカー・救急車・消防車が速やかに現場に到着できるよう、電話番号と住所を対応づけるデータ構造を使う。これは重要だ。電話をかけた人は正確な住所を伝えられないかもしれず、遅れは生死を別つかもしれない。
\end{itemize}

\section{効率の必要性}

次節では多くのデータ構造がサポートする操作を見ていく。ちょっとしたプログラミング経験があれば、これらを正しく実装することは難しくない。データを配列または連結リストに格納し、配列または連結リストの全要素を見てまわり、要素を追加したり削除したりすればよいのだ。

こういう実装は簡単だが、あまり効率的ではない。さて、このことを考える価値はあるだろうか？コンピュータはどんどん高速化している。自明な実装で十分かもしれない。確認のためにざっくりとした計算をしてみよう。

\paragraph{操作の数：}
まあまあの大きさのデータセット、例えば100万（$10^6$）個のアイテムを持つアプリケーションがあるとする。各アイテムを最低一回は参照すると仮定してよいことが多いだろう。つまり少なくとも100万（$10^6$）回はこのデータから検索をしたいわけだ。この$10^6$回の検索それぞれが$10^6$個のアイテムをすべて確認すると、合計$10^6\times 10^6=10^{12}$（1兆）回の確認処理が必要だ。

\paragraph{プロセッサの速度：}
本書の執筆の時点で、かなり高速なデスクトップコンピュータでも毎秒10億（$10^9$）回以上の操作は実行できない。\footnote{コンピュータの速度はせいぜい数ギガヘルツ（数十億回/秒）であり、各操作に普通は数サイクルが必要だ。}よってこのアプリケーションの完了には少なくとも$10^{12}/10^9=1000$秒、すなわち約16分40秒かかる。コンピューターの時間では16分は非常に長いが、人は気にしないかもしれない。（例えば、プログラマがコーヒーブレイクに向かうならそれで構わないだろう。）

\paragraph{大きなデータセット：}
Google\index{Google|}を考えてみよう。Googleでは85億ものWebページからの検索を扱う。先ほどの計算では、このデータに対する問い合わせには少なくとも8.5秒かかる。だが実際にはそんなに時間がかからないことを知っているだろう。Web検索には8.5秒もかからないし、あるページがインデックスに含まれているかよりさらに複雑な問い合わせも実行する。執筆時点でGoogleは1秒間に約$4,500$クエリを受けつける。つまり少なくとも$4,500 \times 8.5 = 38,250$ものサーバーが必要なのだ。

\paragraph{解決策：}
以上の例から、アイテムの数#n#と実行される操作数$m$が共に大きくなると、データ構造の自明な実装はスケールしないことがわかる。今の例で、（機械命令数で数えた）時間はおよそ$#n#\times m$だ。

解決策はもちろん、データ構造内のデータを上手に並べ、各操作がすべてのデータを見て回る必要がないようにすることだ。一見そんなことはムリなように思えるが、データ構造に格納されているデータの数とは関係なく、平均して2つのデータだけを参照すればすむデータ構造をのちに紹介する。 1秒あたり10億命令を実行できるとして、10億個のデータ（兆、京、垓であっても）を含むデータ構造を検索するのにわずか$0.000000002$秒しかかからないのだ。

データ構造内のデータを整列することで、データ数に対して参照されるデータ数が非常にゆっくり増加するデータ構造も後で紹介する。例えば10億個のアイテムを整列しておけば、最大60個のアイテムを参照することで各操作を実行できる。毎秒10億命令実行できるコンピュータでは、これらの操作は$0.00000006$秒しかかからない。

この章の残りの部分では、この本を通して使う主な概念の一部を簡単に解説する。\secref{interface}は本書で説明するデータ構造で実装されるインターフェースを全て説明するので読む必要があると考えて欲しい。残りの節では、以下のものを解説する。
\begin{itemize}
\item 指数・対数・階乗関数や漸近（ビッグオー）記法・確率・ランダム化などの数学の復習
\item 計算のモデル
\item 正しさと実行時間、メモリ使用量
\item 残りの章の概要
\item サンプルコードと組版の規則
\end{itemize}
これらの背景知識があってもなくても、いったん気軽に飛ばし必要に応じて戻り読みしてもよい。

\section{インターフェース}
\seclabel{interface}
データ構造について議論するときは、データ構造のインターフェースと実装の違いを理解することが重要だ。インターフェースはデータ構造が何をするかを、実装はデータ構造がどうやるかを示す。

% TALK caprice 抽象データ型 (英：abstract data types)というふうに固有名詞であることを日英表記で強調したほうがよいのではないか。この本は固有名詞を太字にしないきらいがある。初学者は「抽象的な」「データの」型って？と形容詞のように読んでしまうかもしれない
\emph{インターフェース}\index{interface}\index{abstract data type|see{interface}}（\emph{抽象データ型}と呼ばれることもある）は、データ構造がサポートする操作一式とその意味を定義する。インターフェースを見ても操作がどう実装されているかはわからない。サポートする操作の一覧とその引数、返り値の特徴だけを教えてくれる。 % types of arguments -> 型だけでなく色々な特徴がある

一方でデータ構造の\emph{実装}には、データ構造の内部表現と、操作を実装するアルゴリズムの定義が含まれる。そのため、ひとつのインターフェースに対して複数の実装が考えられる。例えば本書では\chapref{arrays}では配列を、\chapref{linkedlists}ではポインタを使った#List#インターフェースの実装を紹介する。それぞれ同じ#List#インターフェースを実装しているが、実装方法は異なるのだ。

\subsection{#Queue#、#Stack#、#Deque#インターフェース}

#Queue#インターフェースは要素を追加したり、取り出し規則に従って次の要素を削除したりできる要素の集まりを表す。より正確には、#Queue#インターフェースがサポートする操作は以下のものだ。

\begin{itemize}
  \item #add(x)#：値#x#を#Queue#に追加する。
  \item #remove()#：（以前に追加された）次の値#y#を#Queue#から削除し、#y#を返す。
\end{itemize}

#remove()#操作の引数はないことに注意する。#Queue#は\emph{取り出し規則}に従って次に削除する要素を決める。取り出し規則は色々と考えられるが、主なものとしてはFIFOや優先度、LIFOなどがある。 % queueing discipline = 取り出し規則

\figref{queue}に示す\emph{FIFO（first-in-first-out、先入れ先出し）#Queue#}\index{FIFO queue} \index{queue!FIFO}は、追加したのと同じ順番で要素を削除する。これはコンビニのレジで並ぶ列と同じようなものだ。これは最も一般的な#Queue#なので、修飾子のFIFOは省略されることが多い。他の教科書ではFIFO #Queue#の#add(x)#、#remove()#は、それぞれ#enqueue(x)#、#dequeue()#と呼ばれていることも多い。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/queue}}
  \caption{FIFOキュー}
  \figlabel{queue}
\end{figure}

% TALK caprice 同点要素は「同じ優先度を持つ要素」に言い換えた、一般的な用語ではないので。
\figref{prioqueue}に示す優先度付き#Queue#は、
\index{priority queue}%
\index{priority queue|seealso{heap}}%
\index{queue!priority}%
#Queue#から最小の要素を削除する。同じ優先度を持つ要素が複数あるときは、そのうちのいずれかを任意に選ぶ。これは病院の救急室で重症患者を優先的に治療することに似ている。患者が到着したら、症状を見積もってから待合室で待機していてもらう。医師の手が空くと、最も重篤な患者から治療する。優先度付き#Queue#における#remove()#操作を、他の教科書ではよく#deleteMin()#などと呼んでいる。 % TODO: YJ I have never seen deleteMin though.

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/prioqueue}}
  \caption{優先度付きキュー}
  \figlabel{prioqueue}
\end{figure}

よく使う取り出し規則は、\figref{stack}に示すLIFO（last-in-first-out、後入れ先出し）
\index{LIFO queue}%
\index{LIFO queue|seealso{stack}}%
\index{queue!LIFO}%
\index{stack}%
だ。 \emph{LIFOキュー}では、最後に追加された要素が次に削除される。これは皿を積むように視覚化するとよい。皿はスタックの上に積まれ、上から順に持って行かれる。この構造はとてもよく見かけるので#Stack#という名前が付いている。#Stack#について話すとき、#add(x)#、#remove()#のことを、#push(x)#、#pop()#と呼ぶ。こうすればLIFOとFIFOの取り出し規則を区別できる。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/stack}}
  \caption{スタック}
  \figlabel{stack}
\end{figure}

#Deque#（双方向キュー）はFIFOキューとLIFOキュー（スタック）の一般化だ。 #Deque#は先頭と末尾のある要素の列を表し、列の先頭または末尾に要素を追加できる。 #Deque#操作の名前はわかりやすく、#addFirst(x)#、#removeFirst()#、#addLast(x)#、 #removeLast()#だ。スタックは#addFirst()#と#removeFirst()#だけを使えば実装できることは知っておくと良いだろう。一方FIFOキューには#addLast(x)#と#removeFirst()#を使えばよい。 % back = 末尾?

\subsection{#List#インターフェース：線形シーケンス}

この本では、FIFO #Queue#や#Stack#、#Deque#のインターフェースについての話はほとんどしない。なぜなら、これらのインターフェースは#List#インターフェースにまとめられるためだ。図1.4に示す#List#\index{List@#List#}は、値の列$#x#_0,\ldots,#x#_{#n#-1}$を表現する。
#List#インターフェースは以下の操作を含む。

\begin{enumerate}
  \item #size()#: リストの長さ#n#を返す。
  \item #get(i)#: $#x#_{#i#}$の値を返す。
  \item #set(i,x)#: $#x#_{#i#}$の値を#x#にする。
  \item #add(i,x)#: #x#を#i#番目として追加し、$#x#_{#i#},\ldots,#x#_{#n#-1}$をずらす。\\
    すなわち、$j\in\{#i#,\ldots,#n#-1\}$について$#x#_{j+1}=#x#_j$とし、#n#をひとつ増やし、$#x#_i=#x#$とする。
  \item #remove(i)#: $#x#_{#i#}$を削除し、$#x#_{#i+1#},\ldots,#x#_{#n#-1}$をずらす。\\ 
    すなわち、$j\in\{#i#,\ldots,#n#-2\}$について$#x#_{j}=#x#_{j+1}$とし、#n#をひとつ減らす。
\end{enumerate}

これらの操作を使うことで#Deque#インターフェースが実装できる。 % sufficient -> 十分という意味

\begin{eqnarray*}
  #addFirst(x)# &\Rightarrow& #add(0,x)# \\
  #removeFirst()# &\Rightarrow& #remove(0)#  \\
  #addLast(x)# &\Rightarrow& #add(size(),x)# \\
  #removeLast()# &\Rightarrow& #remove(size()-1)#
\end{eqnarray*}

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/list}}
  \caption{#List#は$0,1,2,\ldots,#n#-1$で添え字づけられた列を表現する。この#List#で#get(2)#を実行すると値$c$が返ってくる。}
  \figlabel{list}
\end{figure}


この後の章では#Stack#、#Deque#、FIFO #Queue#のインターフェースについての話はほぼ出てこない。しかし、#Stack#と#Deque#という用語を「#List#インターフェースを実装したデータ構造」の名前として後の章で使うことがある。その場合、#Stack#と#Deque#という名前のついたそれらのデータ構造は、#Stack#と#Deque#のインターフェースを非常に効率良く実装することにも使える、という事実を強調している。たとえば#ArrayDeque#クラスは#List#インターフェースの実装だ。これはすべての#Deque#操作をひとつだけの定数時間操作で実装できる。
% caprice unordered って入れたほうが USetのUが楽に理解できる
\subsection{#USet#インターフェース：順序付けられていない(unordered)要素の集まり}

#USet#\index{USet@#USet#}インターフェースは重複がなく、順序付けられていない要素の集まりを表現する。これは数学における\emph{集合}を模したものだ。#USet#には、#n#個の\emph{互いに相異なる}要素が含まれる。つまり、同じ要素が複数入っていることはない。また、要素の並び順は決まっていない。#USet#は以下の操作をサポートする。

\begin{enumerate}
\item #size()#：集合の要素数#n#を返す。
\item #add(x)#：要素#x#が集合に入っていなければ集合に追加する。\\
$#x# = #y#$を満たす集合の要素#y#が存在しないなら、集合に#x#を加える。#x#が集合に追加されたら#true#を返し、そうでなければ#false#を返す。
\item #remove(x)#：集合から#x#を削除する。\\
$#x# = #y#$を満たす集合の要素#y#を探し、集合から取り除く。そのような要素が見つかれば#y#を、見つからなければ#null#を返す。
\item #find(x)#：集合に#x#が入っていればそれを見つける。\\
$#x# = #y#$を満たす集合の要素#y#を見つける。そのような要素が見つかれば#y#を、見つからなければ#null#を返す。
\end{enumerate}

今述べた定義で、探したい#x#と、見つかるかもしれない集合の要素#y#の区別を小難しく感じるかもしれない。これを区別する理由は、#x#と#y#は実は異なるオブジェクトだが等しいと判定されるかもしれないためだ。\javaonly{\footnote{Javaでは、クラスの#equals(y)#・#hashCode()#メソッドをオーバーライドするとこれを行える。}}異なるオブジェクトを等しいと判定するインターフェースは、キーを値に対応づける\emph{辞書}(\emph{マップ})を作るのに便利なのだ。 % TODO: YJ need better translation
\index{dictionary}%
\index{map}%
% TODO 辞書、マップとハッシュテーブルの関わりを脚注で補足する
辞書(マップ)を作るために、まず#Pair#\index{pair}という複合オブジェクトを作る。#Pair#は\emph{キー}と\emph{値}からなる。 2つの#Pair#は、キーが等しいければ等しいとみなされる。#Pair#である$(#k#,#v#)$を#USet#に入れてから、$#x#=(#k#,#null#)$として#find(x)#を呼び出すと、$#y#=(#k#,#v#)$が返ってくる。すなわち、キー#k#だけから値#v#を復元できるのだ。

\subsection{#SSet#インターフェース：ソートされた(sorted)要素の集まり}
\seclabel{sset}
% TODO caprice 全順序の脚注を入れる
\index{SSet@#SSet#}%
#SSet#インターフェースは順序づけされた要素の集まりを表現する。#SSet#は全順序な要素を格納するので、任意の2つの要素#x#と#y#は比較可能である。サンプルコードでは、以下のように定義される#compare(x, y)#メソッドで比較を行うものとする。

\[
    #compare(x,y)#
      \begin{cases}
        {}<0 & \text{if $#x#<#y#$} \\
        {}>0 & \text{if $#x#>#y#$} \\
        {}=0 & \text{if $#x#=#y#$}
      \end{cases}
\]
\index{compare@#compare(x,y)#}%

#SSet#は#USet#と全く同じセマンティクスを持つ操作#size()#、#add(x)#、#remove(x)#をサポートする。#USet#と#SSet#の違いは#find(x)#にある。 % セマンティクス vs. 意味

\begin{enumerate}
\setcounter{enumi}{3}
\item #find(x)#: 順序づけられた集合から#x#の位置を特定する。\\
   $#y# \ge #x#$を満たす最小の要素#y#を探す。
   このような#y#が存在すればそれを返し、そうでないなら#null#を返す。
\end{enumerate}

この#find(x)#は、\emph{後継探索(XXX:訳語)}\index{successor search}と呼ばれることがある。#x#に等しい要素がなくても意味のある結果を返す点で#USet.find(x)#とは異なる。

#USet#、#SSet#における#find(x)#の区別は重要なのだが見過ごされがちである。 #SSet#は追加の仕事をしてくれるぶん、実装が複雑で実行時間が長くなりがちだ。例えば、この本で述べる#SSet#の#find(x)#の実装では、要素数の対数だけの時間がかかる。一方、\chapref{hashing}の#ChainedHashTable#による#USet#の実装では、#find(x)#の実行時間の期待値は定数である。#SSet#の追加機能が本当に必要でないなら、#SSet#ではなく常に#USet#を使うべきだ。

\section{数学的背景}
この節では本書で使用する数学記法や基礎知識を復習する。例えば、対数やビッグオー記法、確率論などだ。内容はあっさりしたもので、丁寧な手解きはしない。背景知識が足りないと感じる読者のために、コンピュータサイエンスのための数学の優れた無料の教科書がある。必要に応じて適切な箇所を読み、練習問題を解いてみるとよいだろう。
\cite{llm11}.

\subsection{指数と対数}

\index{exponential}
$b^x$と書いて$b$の$x$乗を表す。$x$が正の整数なら、$b$にそれ自身を$x-1$回掛けた値になる。

\[
    b^x = \underbrace{b\times b\times \cdots \times b}_{x}
\]

$ x $が負の整数なら、$b^x=1/b^{-x}$である。$x=0$なら、$b^x=1$である。
$b$が整数でないときも、やはり（後述する）指数関数$e^x$の観点から、べき乗を定義できる。指数関数もまた指数級数を使って定義されているが、こういう話は微積分の教科書に任せることにする。

\index{logarithm}
この本では$\log_b k$と書いて\emph{$b$を底とする対数}を表す。これは次の式を満たす$x$として一意に決まる、

\[
    b^{x} = k
\]

この本に出てくる対数の底はほとんどの場合2である。底が2の対数を\emph{二進対数}という。そのため、底になにも書かない$\log k$は$\log_2 k$の省略記法とする。
\index{binary logarithm}%
\index{logarithm!binary}%
% TALK caprice 二分探索（英：binary search）と強調したい
対数の大雑把だが分かりやすいイメージを紹介する。$\log_b k$とは$k$を何回$b$で割ると1以下になるかを表す数だと考えればよい。例を挙げよう。二分探索という手法を使うと、一回の比較処理のたびに、答えの候補の個数が半分になる。答えの候補が1つに絞られるまで、この処理を繰り返す。$n+1$個の答えの候補が最初にあるなら、二分検索に必要な比較の回数は$\lceil \log_2(n+1) \rceil$以下だ。
% TODO caprice ガウス記号の脚注をココにいれる

\index{natural logarithm}%
\index{logarithm!natural}%
この本で\emph{自然対数}という別の対数も何度か出てくる。$\ln k$と書いて$\log_e k$を表すことにする。ここで、$e$は次のように定義される\emph{オイラーの定数}だ。
\index{Euler's constant}%
\index{e@$e$ (Euler's constant)}%
\[
   e = \lim_{n\rightarrow\infty} \left(1+\frac{1}{n}\right)^n
   \approx  2.71828
\]

自然対数は頻繁に現れる。これは$e$がよく見かける次のような積分値であるためだ。
\[
    \int_{1}^{k} 1/x\,\mathrm{d}x  = \ln k
\]

よく使う対数の操作は2つある。ひとつは指数部からの取り出し操作だ。
\[
    b^{\log_b k} = k
\]

もう一つは底の取り替え操作だ。
\[
    \log_b k = \frac{\log_a k}{\log_a b}
\]

これら2つの操作を使えば、例えば自然対数と二進対数を比較できる。
\[
   \ln k = \frac{\log k}{\log e} = \frac{\log k}{(\ln e)/(\ln 2)} =
    (\ln 2)(\log k) \approx 0.693147\log k
\]

\subsection{階乗}
\seclabel{factorials}

\index{factorials}
この本で\emph{階乗関数}を使う箇所がいくつかある。$n$が非負整数のとき、$n!$（ 「$n$の階乗」と読む）は次のように定義される。
\[
   n! = 1\cdot2\cdot3\cdot\cdots\cdot n
\]

$n!$は$n$要素の相異なる順列\index{permutation}の個数である。つまり$n$個の相異なる要素の並べ方の数として階乗は現れる。なお、$n=0$のときについて、$0!$は1と定義される。

\index{Stirling's Approximation}%
$n!$の大きさは、\emph{スターリングの近似}によって近似的に求められる。
\footnote{訳注：以下、スターリングの近似に関する議論は、初学者は飛ばしても良いと思われる。}
\[
  n!
   = \sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}e^{\alpha(n)}
\]
ここで$\alpha(n)$は次の条件を満たす。
\[
   \frac{1}{12n+1} <  \alpha(n) < \frac{1}{12n}
\]

スターリングの近似を使って$\ln(n!)$の近似値も計算できる。
\[
   \ln(n!) = n\ln n - n + \frac{1}{2}\ln(2\pi n) + \alpha(n)
\]
% TALK caprice このカッコ内の記述についても「初学者は無視して良い」と脚注入れたい
（スターリングの近似を証明する簡単な方法として、$\ln(n!)=\ln 1 + \ln 2  + \cdots + \ln n$を$\int_1^n \ln n\,\mathrm{d}n = n\ln n - n +1$で近似するというものがある。)

\index{binomial coefficients}%
\emph{二項係数}は階乗関数とつながりがある。$n$を非負整数、$k$を$\{0,\ldots,n\}$の要素とするとき、$\binom{n}{k}$は次のように定義される。

\[
   \binom{n}{k} = \frac{n!}{k!(n-k)!}
\]
二項係数$\binom{n}{k}$は、大きさ$n$の集合における大きさ$k$の部分集合の個数である。すなわち、集合$\{1,\ldots,n\}$から相異なる$k$個の整数を取り出すときの場合の数である。 % TODO: YJ 元の文でもi.e.とあるが、これは定義ではなく一つの例なのでは？すなわち、と言ってしまってよいのだろうか。「例えば」の方が正確では。

\subsection{漸近記法}

\index{asymptotic notation} \index{big-Oh notation} \index{O@$O$ notation}
データ構造を分析する際には操作の実行時間についての議論が有用だ。正確な実行時間はコンピュータごとに異なる。同じコンピュータで実行する場合ですらバラつくだろう。ここで操作の実行時間とは操作に必要なコンピュータ命令数を指す。単純なコードであっても、この量を正確に計算するのは難しいことがある。そのため実行時間を正確に解析するのではなく、\emph{ビッグオー記法}と呼ばれる記法を使う。$f(n)$を関数とするとき、$O(f(n))$は次のような関数の集合を表す。
\[
   O(f(n)) = \left\{
     \begin{array}{l}
       g(n):\mbox{ある$c>0$と$n_0$が存在し、} \\
             \quad\mbox{任意の$n\ge n_0$について$g(n) \le c\cdot f(n)$を満たす}
     \end{array} \right\}
\]
グラフ上で考えると、$n$が十分に大きくなると$c\cdot f(n)$が$g(n)$よりも大きくなるか同じになる関数$g(n)$を集めたものがこの集合だ。 % 「上から抑えられる」は既に知識がある人には伝わるが、初学者には伝われないのでは。 % TODO: YJ need revision

漸近表記を使えば関数を単純化できる。たとえば、$5n\log n + 8n - 200$の代わりに$O(n \log n)$と書ける。これは次のように証明できる。
\begin{align*}
       5n\log n + 8n - 200
        & \le 5n\log n + 8n \\
        & \le 5n\log n + 8n\log n & \mbox{ $n\ge 2$のとき（このとき$\log n \ge 1$）}
            \\
        & \le 13n\log n
\end{align*}
$ c = 13 $および$ n_0 = 2 $とすれば、関数$ f(n)= 5n \log n + 8n-200 $が集合$ O(n \log n)$に含まれることがわかる。

漸近表記の便利な性質をいくつか挙げる。

まずは、任意の定数$c_1 < c_2$について以下が成り立つ。
\[ O(n^{c_1}) \subset O(n^{c_2}) \]

つづいて、任意の定数$ a, b, c> 0 $について以下が成り立つ。
\[ O(a) \subset O(\log n) \subset O(n^{b}) \subset O({c}^n) \]

これらの包含関係はそれぞれに正の値を掛けても保たれる。
たとえば$n$を掛けると次のようになる。

\[ O(n) \subset O(n\log n) \subset O(n^{1+b}) \subset O(n{c}^n) \]

長く有名な伝統に従い、本書でもビッグオー記法を濫用し、$f_1(n) = O(f(n))$ と書いて$f_1(n) \in O(f(n))$であることを表す。また、「この操作の実行時間は$O(f(n))$に\emph{含まれる}」ことを単に「この操作の実行時間は$O(f(n))$だ」と言う。これらの短い言い方は語感を整え、漸近記法を連続する等式の中で使いやすくするのに役立つ。

この書き方の、奇妙な例を挙げる。
\[
  T(n) = 2\log n + O(1)
\]

これは正確に書くとこうなる。
\[
  T(n) \le 2\log n + [\mbox{$O(1)$のある要素]}
\]

$O(1)$には別の問題もある。この記法には変数が入ってないので、どの変数が大きくなるのかわからないのだ。文脈から読み取る必要がある。上の例では、方程式の中に変数は$n$しかないので、$T(n)= 2 \log n + O(f(n))$の$f(n) = 1$であるものと読み取ることになる。

ビッグオー記法は新しいものでも、コンピュータサイエンス独自のものでもない。1894年には数学者Paul Bachmannが使っていた。その後しばらくしてコンピュータサイエンスにおいてアルゴリズムの実行時間を論ずるのに非常に便利なことが判明したのだ。
次のコードを考えてみよう。

\javaimport{junk/Simple.snippet()}
\cppimport{ods/Simple.snippet()}

このメソッドを1回実行すると以下の処理が行われる。
\begin{itemize}
      \item 代入$1$回 (#int\, i\, =\, 0#)
      \item 比較$#n#+1$回 (#i < n#)
      \item インクリメント #n# 回(#i++#)
      \item 配列のオフセット計算 #n# 回 (#a[i]#)
      \item 間接代入#n#回 (#a[i] = i#)
\end{itemize}

よって実行時間は以下のようになる。
\[
    T(#n#)=a + b(#n#+1) + c#n# + d#n# + e#n#
\]

$a$、$b$、$c$、$d$、$e$はプログラムを実行するマシンに依存する定数で、それぞれ代入、比較、インクリメント、配列のオフセット計算、間接代入の実行時間を表す。しかしたった2行のコードの実行時間を表す式がこうだと、より複雑なコードやアルゴリズムをこのやり方では扱えないだろう。ビッグオー記法を使うと、実行時間は次のようにより単純になる。
\[
    T(#n#)= O(#n#)
\]

この書き方はよりコンパクトながらさっきの式と同じくらいのことを教えてくれる。実行時間は定数$a$、$b$、$c$、$d$、$e$に依存している。これらの値がわからないと、実行時間はわからず比較できないのだ。これらの定数を明らかにするため努力しても（例えば実際に時間を測ってみる）、得られる結論はそのマシンにおいてのみ有効なだけだ。

ビッグオー記法を使うと、より高次の分析をすることが出来るのでより複雑な関数も扱うことが出来る。 2つのアルゴリズムのビッグオー記法での実行時間が同じなら、どちらが速いかわからず、はっきりとした勝ち負けがつかないかもしれない。あるマシンでは一方が速く、別のマシンでは他方が速いかもしれない。しかし2つのアルゴリズムのビッグオー記法での実行時間が異なることを示せれば、実行時間が小さい方は\emph{#n#が十分大きければ}速いとわかる。

ビッグオー記法を使って2つの異なる関数を比べる例を\figref{intro-asymptotics}示す。これは$f_1(#n#)=15#n#$と$f_2(n)=2#n#\log#n#$の増加を比べたものだ。$f_1(#n#)$は複雑な線形時間アルゴリズムの実行時間、$f_2(#n#)$は分割統治に基づくシンプルなアルゴリズムの実行時間だ。これを見ると、#n#が小さいうちは$f_1(#n#)$はより大きいが$f_2(#n#)$、#n#が大きくなると逆転することがわかる。そして、最終的には$f_1(#n#)$が圧倒的に性能がよくなるのだ。ビッグオー記法を使った解析で$O(#n#)\subset O(#n#\log #n#)$となることから、このことを知ることができる。

\begin{figure}
  \begin{center}
    \newlength{\tmpa}\setlength{\tmpa}{.98\linewidth}
    \addtolength{\tmpa}{-4mm}
    \resizebox{\tmpa}{!}{\input{images/bigoh-1.tex}}\\[4ex]
    \resizebox{.98\linewidth}{!}{\input{images/bigoh-2.tex}}
  \end{center}
  \caption{$15#n#$対$2#n#\log#n#$のプロット}
  \figlabel{intro-asymptotics}
\end{figure}

複数の変数を持つ関数に対して漸近表記を使用する場合もある。標準的な定義は定まっていないようだが、この本では次の定義を用いる。
% TODO caprice から YJ この定義ビルドしたとき端が切れてるよ　改行が必要
\[
   O(f(n_1,\ldots,n_k)) =
   \left\{\begin{array}{@{}l@{}}
             g(n_1,\ldots,n_k):\mbox{ある$c>0$と$z$が存在し、} \\
             \qquad \mbox{$g(n_1,\ldots,n_k)\ge z$を満たす任意の$n_1,\ldots,n_k$について、}
             \quad \mbox{$g(n_1,\ldots,n_k) \le c\cdot f(n_1,\ldots,n_k)$が成り立つ。} \\
   \end{array}\right\}
\]

この定義を使えば我々が考えたいことを表現することが出来る。引数$n_1,\ldots,n_k$が$g$を大きくするときのことだ。この定義は$f(n)$が$n$の増加関数なら一変数の場合の$O(f(n))$の定義と同じだ。我々の目的のためにはこの定義でよいのだが、多変数の場合の漸近記法を異なる定義を与えている教科書もあることには注意が必要だ。
% TALK 原文がrandomizationなのでランダム性ではなく乱択化では？
\subsection{ランダム性と確率}
\seclabel{randomization}

\index{randomization}%
\index{probability}%
\index{randomized data structure}%
\index{randomized algorithm}%
この本で扱うデータ構造には\emph{ランダム性}を利用するものがある。格納されているデータや実行された操作だけでなく、サイコロの目もふまえて動作を決めるのだ。そのため同じことをしても実行時間は毎回同じであるとは限らない。こういうデータ構造を分析するときは平均または\emph{期待実行時間}を考えるのがよい。
\index{expected running time}%
\index{running time!expected}%

形式的には、ランダム性を利用するデータ構造における操作の実行時間は確率変数である。そしてその\emph{期待値}を知りたい。全事象$U$の値をとる離散確率変数を$X$とするとき、$X$の期待値$E[X]$は以下のように定義される。
\index{expected value}%
\[
    \E[X] = \sum_{x\in U} x\cdot\Pr\{X=x\}
\]

ここで、$\Pr\{\mathcal{E}\}$は事象$\mathcal{E}$の発生確率とする。この本の例では、データ構造の内部で発生するランダム性のみを考慮して確率を定める。データ構造に入ってくるデータや実行される操作列がランダムだという仮定は置かないことに注意する。

期待値の最も重要な性質の一つとして\emph{期待値の線形性}がある。
\index{linearity of expectation}%
任意のふたつの確率変数$X$と$Y$について以下の関係が成り立つ。
\[
   \E[X+Y] = \E[X] + \E[Y]
\]

より一般的には、任意の確率変数$ X_1,\ldots,X_k $について以下の関係が成り立つ。
\[
   \E\left[\sum_{i=1}^k X_i\right] = \sum_{i=1}^k \E[X_i]
\]

期待値の線形性によって、（上の式の左辺のように）複雑な確率変数を(右辺のような)より単純な確率変数の和に分解できる。

\emph{インジケータ確率変数}\index{indicator random variable}はよく使う便利なトリックだ。この二値変数はなにかを数えたいときに役立つ。例を見るとよくわかるだろう。表裏が等しい確率で出るコインを$k$回投げたとき、表が出る回数の期待値を知りたいとする。
\index{coin toss}
直感的な答えは$k/2$だが、期待値の定義を使って証明しようとすると次のようになる。

\begin{align*}
   \E[X] & = \sum_{i=0}^k i\cdot\Pr\{X=i\} \\
         & = \sum_{i=0}^k i\cdot\binom{k}{i}/2^k \\
         & = k\cdot \sum_{i=0}^{k-1}\binom{k-1}{i}/2^k \\
         & = k/2
\end{align*}
% この「２項係数の性質」、原文自体が公式間違ってたのでプルリク送った上でここでも直しといた
この計算は$\Pr\{X=i\} = \binom{k}{i}/2^k$および2項係数の性質$i\binom{k}{i}=k\binom{k-1}{i-1}$や$\sum_{i=0}^{k} \binom{k}{i} = 2^{k}$を知っていないとできない。
% TALK インジケータ確率変数（英：indicator random variables）としたい
インジケータ変数と期待値の線形性を使えばはるかに簡単になる。$\{1,\ldots,k\}$の各$i$に対し以下のインジケータの確率変数を定義する。

\[
    I_i = \begin{cases}
           1 & \text{$i$番目のコイントスの結果が表のとき} \\
           0 & \text{そうでないとき}
          \end{cases}
\]
そして、以下の計算を行う。
\[ \E[I_i] = (1/2)1 + (1/2)0 = 1/2 \]
ここで、$X=\sum_{i=1}^k I_i$なので以下のように所望の値を得られる。
\begin{align*}
   \E[X] & = \E\left[\sum_{i=1}^k I_i\right] \\
         & = \sum_{i=1}^k \E[I_i] \\
         & = \sum_{i=1}^k 1/2 \\
         & = k/2
\end{align*}

この計算は少し長いものの、不思議な等式や非自明な確率計算は必要ない。各コイントスは$1/2$の確率で表が出るので、表を出すコインの数は恐らく試行回数の半分だ、という直感の説明でもある。

\section{計算モデル}
\seclabel{model}

本書ではデータ構造における操作の実行時間を理論的に分析する。これを正確に行うための計算の数学的なモデルが必要だ。そのために\emph{#w#ビットのワードRAM}モデルを使うことにする。
\index{word-RAM}%
\index{RAM}%
RAMはランダムアクセスマシン(Random Access Machine)の頭字語である。
このモデルではランダムアクセスメモリを使える。
ランダムアクセスメモリはセルの集まりで、これはそれぞれ#w#ビットのワードを格納できる。
\index{word}
つまり、各セルは数値集合$\{0,\ldots,2^{#w#}-1\}$の中のひとつの数を表せる。
% TODO 剰余の脚注
ワードRAMモデルではワードの基本的な操作に一定の時間が必要である。基本的な操作は算術演算（#+#, #-#, #*#, #/#, #%#）や比較（$<$, $>$, $=$, $\le$, $\ge$）、ビット単位の論理演算（ビット単位の論理積 ANDや論理和 OR、排他的論理和 XOR）である。

どのセルも定数時間で読み書きできる。コンピュータのメモリはメモリ管理システムによって管理される。メモリ管理システムは必要に応じてメモリブロックを割り当てたり割り当て解除したりしてくれる。サイズ$k$のメモリブロックの割当てには$O(k)$の時間がかかり、新しく割り当てられたメモリブロックへの参照（ポインタ）が返される。この参照はひとつのワードで表せる程度小さい。

ワード幅#w#はこのモデルの重要なパラメータである。
この本で#w#に置く仮定は、#n#をデータ構造に格納されうる要素数とするとき、$#w# \ge \log #n#$であるということだけだ。これは控えめな仮定である。なぜならこれが成り立たないとひとつのワードではデータ構造の要素数を数えることすらできないためである。

領域はワード単位で測るので、データ構造で使う領域の広さとはメモリの使用するワード数のことである。我々のデータ構造はみなジェネリック型#T#の値を格納し、#T#型の要素は1ワードのメモリで表現できると仮定する。
\javaonly{（実際に、Javaでは#T#型のオブジェクトの参照を格納しており、この参照は1ワードのメモリを占める。）}

\javaonly{#w#ビットのワードRAMモデルは、$#w#=32$とすると、（32ビット）Java仮想マシン（JVM）と非常に近い。}
\cpponly{#w#ビットのワードRAMモデルは、$#w#=32$または$#w#=64$とすると、現代のデスクトップコンピュータ環境と非常に近い。}
この本に載っているデータ構造は、実装できないような特殊なトリックを使ってはいない。

\section{正しさ、時間複雑性、空間複雑性}

データ構造の性能を考えるとき重要な項目が3つある。
% TALK caprice ここも英語を加えたい
\begin{description}
  \item[正しさ：]データ構造はそのインタフェースを正しく実装しなければならない。
  \item[時間複雑性：]データ構造における操作の実行時間は短いほどよい。
  \item[空間複雑性：]データ構造のメモリ使用量は小さいほどよい。
\end{description}

この本は入門書なので正しさを満たすデータ構造しか扱わない。つまり、不正確な出力が得られることがあったり、更新をちゃんとしなかったりするデータ構造のことは考えない。
一方で、メモリ使用量を最小限に抑えるための工夫をしているデータ構造は紹介する。
これは操作の（漸近的な）実行時間には影響しないことが多いが、実用上ではデータ構造を少し遅くするかもしれない。

データ構造の実行時間を考えるとき、3つの異なる実行時間保証を扱うことがよくある。

\begin{description}
\item[最悪実行時間：]
  \index{running time}%
  \index{running time!worst-case}%
  \index{worst-case running time}%
  これは最も強力な実行時間の保証である。操作の最悪実行時間が$f(#n#)$ならば、操作の実行時間は\emph{決して}$f(#n#)$よりも長いことはない。
\item[償却実行時間：]
  \index{running time!amortized}%
  \index{amortized running time}%
  償却実行時間が$f(#n#)$ならば、典型的な操作のコストが$f(#n#)$であることを意味する。
  より正確には、$m$個の操作の列が$mf(#n#)$であることを意味する。
  いくつかの操作には、個別では$f(#n#)$よりも長い時間がかかるかもしれないが、操作の列全体として考えると、ひとつあたりのコストは$f(#n#)$以下なのである。 % TODO: YJ amortize = 償却: better translation?
\item[期待実行時間：]
  \index{running time!expected}%
  \index{expected running time}%
  期待実行時間が$f(#n#)$ならば、実際の実行時間は確率変数（\secref{randomization}を参照）であり、この確率変数の期待値が$f(#n#)$である。
  ここでいうランダム性はデータ構造が行う選択のランダム性を指す。
\end{description}

最悪、償却、期待実行時間の違いを理解するのには、お金の例え話が役に立つ。家を買う費用のことを考えてみよう。 % finance

\paragraph{最悪コストと償却コスト}
\index{amortized cost}%
家の価格が120000ドルだとする。毎月1200ドルの120ヶ月（10年）の住宅ローンでこの家が手に入るかもしれない。この場合、月額費用は最悪でも月1200ドルだ。

十分な現金を持っていれば120000ドルの一括払いで家を買うこともできる。こうするとこの家を購入代金を10年で償却した月額費用は以下のようになる。
\[
   \$120\,000 / 120\text{ヶ月} = \$1\,000\text{月あたり}
\]

これはローンの場合に支払う月額1200ドルよりだいぶ少ない。

\paragraph{最悪コストと期待コスト}
次に、12万ドルの家における火災保険を考えてみよう。保険会社が何十万件もの事例を調べた結果、大多数の家は火事を起こさず、わずかな数の家が煙による被害程度で済むボヤを起こし、ごく少数の家が全焼被害になることを把握した。保険会社はこの事例情報に基づき、12万ドルの家における火災被害額の期待値は月額10ドル相当であると判断し、火災保険として儲けるために月額15ドルの料金設定を行なった。

決断のときだ。15ドルを最悪支払月額（かつ期待支払月額）とするその会社の火災保険に入るべきだろうか？それとも、いちかばちか、期待支払月額である月額10ドルを自分で貯める自家保険を行うことにして
\footnote{訳注：なぜ被保険者側が、保険会社しか知らないはずのその額を知っているのかは不問とする。}
、月額5ドルの節約を選ぶべきだろうか？期待支払月額としては明らかに自家保険の方が安いが、実際支払月額が遥かに高くなる可能性を考慮しなければならない。すなわち自家保険では、低い確率ではあるが、家が全焼して実際支払月額が最悪支払月額である12万ドルになる確率があるのだ。

この例は、我々がどちらを選ぶかは場合によって変わるのだ、と教えてくれる。
\footnote{訳注：この例では、最悪支払月額の上限が大幅に低くなることを考慮して、火災保険に入ることを選ぶ人が多いかもしれない。しかし驚くべきことに、\emph{データ構造の世界では、最悪実行時間よりも償却・期待実行時間が低いことを優先する}ことの方が遥かに多い。というのも、最悪実行時間だけかかったときの損害が全焼ほど大きくはなく、またその確率も全焼よりはるかに小さく制御できることが多いからだ。}
償却・期待実行時間は最悪実行時間よりも小さいことが多い。最悪実行時間の長さに目をつむり、償却・期待実行時間でいえば短いことで妥協することにすれば、はるかに単純なデータ構造を採用できる場合がよくあるのだ。

\section{コードサンプル}
\pcodeonly{
この本のコードサンプルは擬似コードで書いた。
\index{pseudocode}%
ここ40年のどの一般的なプログラミング言語の経験を持つ人も理解しやすいコードを書いたつもりである。
この本におけるコードがどんなものかは、配列#a#の平均値を計算する次の擬似コードを見てみるとわかるだろう。
\pcodeimport{ods/Algorithms.average(a)}
このコードでは、変数への代入を表す記法は$\gets$である。
% WARNING: graphic typesetting of assignment operator
配列#a#の大きさを#len(a)#と書き、配列の添え字は0からはじまることにする。
このとき、#range(len(a))#は#a#の正しい添え字の集まりである。
コードを短くし、また読みやすくするために、部分配列代入を使うことがある。
次のふたつの関数は同じことをしている。
\pcodeimport{ods/Algorithms.left_shift_a(a).left_shift_b(a)}
続くコードは配列のすべての要素を0にするものである。
\pcodeimport{ods/Algorithms.zero(a)}
このようなコードの実行時間を解析するときは#a[0:len(a)] = 1#や、#a[1:len(a)] = a[0:len(a)-1]#のような文は、定数時間では実行できないことに気をつけなければならない。
これらの実行時間は$O(#len(a)#)$である。

変数の代入においても同様の簡略記法を使う
#x,y=0,1#は、#x#を0に#y#を1にする。
また#x,y = y,x#は変数#x#と#y#の値を入れ替える。
\index{swap}

この本の擬似コードには、親しみがないかもしれない記法も小数だが使われている。
数学における（ふつうの）割り算の演算子を$/$と書く。
整数の除算を使う必要のある場面も多く、これを$#//#$演算子で表す。
$#a//b# = \lfloor a/b\rfloor$は、$a/b$の整数部分である。
そのため例えば、$3/2=1.5$だが、$#3//2# = 1$である。
\index{integer division}%
\index{div operator}%
整数除算における余りを計算する$\bmod$演算子を使うこともあるが、これはその際になってから定義することにする。
\index{mod operator}%
\index{div operator}%
ビット単位の演算子を使うこともある。
具体的には、左シフト（#<<#）、右シフト（#>>#）、ビット単位の論理積（#&#）、そしてビット単位の排他的論理和（#^#）である。
\index{left shift}%
\index{#<<#|see {left shift}}%
\index{right shift}%
\index{#>>#|see {right shift}}%
\index{bitwise and}%
\index{#&#|see {bitwise and}}%
\index{#^#|see {bitwise exclusive-or}}%

この本の擬似コードはPythonのコードから機械的に翻訳したものである。
元となったコードは本のウェブサイトからダウンロードできる。
\footnote{ \url{http://opendatastructures.org}}である。もし擬似コードにおいて曖昧な点があれば、対応するPythonのコードを参照することができる。もしPythonが読めなければ、JavaとC++で書かれたコードもある。擬似コードの意味がわからず、PythonもC++もJavaも読めないなら、まだこの本を読むのは早いかもしれない。}

\notpcode{
この本のコードサンプルは\lang{}で書いた。
しかし、\lang{}に親しみのない人も読めるようシンプルに書いたつもりだ。
例えば#public#や#private#は出てこない。オブジェクト指向を前面に押し出すこともない。

B、C、C++、C\#、Objective-C、D、Java、JavaScriptなどをALGOL系の言語書いたことのある人は本書のコードを見て意味がわかるだろう。
完全な実装に興味のある読者はこの本に付属の\lang{}ソースコードを見てほしい。

この本は数学的な実行時間の解析と、対象のアルゴリズムを実装した\lang{}のコードとを共に含む。そのためソースコードと数式で同じ変数が出てくる。
このような変数は同じ書式で書く。
一番よく出てくるのは変数#n#\index{n@#n#}である。
#n#は常にデータ構造に格納されている要素の個数を表すものとする。
}

\section{データ構造の一覧}

表~\ref{tab:summary-i}と表~\ref{tab:summary-ii}は本書で扱うデータ構造における性能の要約である。これらは\secref{interface}で説明した#List#や#USet#、#SSet#を実装する。
\Figref{dependencies}はこの本の各章の依存関係を示している。
\index{dependencies}%
破線の矢印は弱い依存関係を示している。これは章のごく小さいが依存や、一部の結果のみに依存することを示す。

\begin{table}
\vspace{56pt}
\begin{center}
\resizebox{.98\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{#List#の実装} \\ \hline
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# & \\ \hline
#ArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A} & \sref{arraystack} \\
#ArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{arraydeque} \\
#DualArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{dualarraydeque}\\
#RootishArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A}  & \sref{rootisharraystack} \\
#DLList# & $O(1+\min\{#i#,#n#-#i#\})$ & $O(1+\min\{#i#,#n#-#i#\})$  & \sref{dllist} \\
#SEList# & $O(1+\min\{#i#,#n#-#i#\}/#b#)$ & $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$\tnote{A}  & \sref{selist} \\
#SkiplistList# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E}  & \sref{skiplistlist} \\ \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{#USet#の実装} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#ChainedHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{hashtable} \\ 
#LinearHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{linearhashtable} \\ \hline
\end{tabular}
\begin{tablenotes}
\item[A]{\emph{償却}実行時間を表す。}
\item[E]{\emph{期待}実行時間を表す。}
\end{tablenotes}
\end{threeparttable}}
\end{center}
\caption{#List#・#USet#の実装の要約}
\tablabel{summary-i}
\end{table}

\begin{table}
\begin{center}
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{#SSet#の実装} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#SkiplistSSet# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{skiplistset} \\ 
#Treap# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{treap} \\ 
#ScapegoatTree# & $O(\log #n#)$ & $O(\log #n#)$\tnote{A} & \sref{scapegoattree} \\
#RedBlackTree# & $O(\log #n#)$ & $O(\log #n#)$ & \sref{redblacktree} \\ 
#BinaryTrie#\tnote{I} & $O(#w#)$ & $O(#w#)$ & \sref{binarytrie} \\ 
#XFastTrie#\tnote{I} & $O(\log #w#)$\tnote{A,E} & $O(#w#)$\tnote{A,E} & \sref{xfast} \\ 
#YFastTrie#\tnote{I} & $O(\log #w#)$\tnote{A,E} & $O(\log #w#)$\tnote{A,E} & \sref{yfast} \\ 
\javaonly{#BTree# & $O(\log #n#)$ & $O(B+\log #n#)$\tnote{A} & \sref{btree} \\ 
#BTree#\tnote{X} & $O(\log_B #n#)$ & $O(\log_B #n#)$ & \sref{btree} \\ } \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{(Priority) #Queue# implementations} \\ \hline
 & #findMin()# & #add(x)#/#remove()# & \\ \hline
#BinaryHeap# & $O(1)$ & $O(\log #n#)$\tnote{A} & \sref{binaryheap} \\ 
#MeldableHeap# & $O(1)$ & $O(\log #n#)$\tnote{E} & \sref{meldableheap} \\ \hline
\end{tabular}
\begin{tablenotes}
\item[I]{このデータ構造は#w#ビット整数のみを格納できる}
\javaonly{\item[X]{これは外部メモリモデルでの実行時間である。\chapref{btree}を参照せよ。}}
\end{tablenotes}
%\renewcommand{\thefootnote}{\arabic{footnote}}
\end{threeparttable}
\end{center}
\caption{#SSet#・優先度付き#Queue#の実装の要約}
\tablabel{summary-ii}
\end{table}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/dependencies}
  \end{center}
  \caption{この本の内容の依存関係}
  \figlabel{dependencies}
\end{figure}

\section{ディスカッションと練習問題}

\secref{interface}で説明した#List#・#USet#・#SSet#インターフェースは、 Java Collections Framework\cite{oracle_collections}の影響を受けている。
\index{Java Collections Framework}%
これらはJava Collections Frameworkの#List#・#Set#・#Map#・#SortedSet#・#SortedMap#をシンプルにしたものである。
\javaonly{付属のソースコードは#USet#・#SSet#の実装を#Set#・#Map#・#SortedSet#・#SortedMap#の実装にするためのラッパークラスを含んでいる。}

この章で扱った漸近記法・対数・階乗・スターリングの近似・確率論の基礎などは、Leyman, Leighton, and Meyer\cite{llm11}の素晴らしい（そしてフリーの）本が扱っている。
分かりやすい微積分の教科書としては、無料で手に入るThompson\cite{t14}の古典的な教科書がある。この本では指数や対数の形式的な定義が書かれている。

基礎的な確率論については、特にコンピュータ・サイエンスに関連するものとしてRoss\cite{r01}の教科書がおすすめである。
漸近記法や確率論などを含むGraham, Knuth, and Patashnik\cite{gkp94}の教科書も参考になるだろう。

\javaonly{Javaプログラミング力を磨きたい読者のためには、オンラインのJavaのチュートリアル~\cite{oracle_tutorials}がある。}

\begin{exc}
練習問題は読者が問題に対する正しいデータ構造を選ぶ練習をするためのものだ。
利用可能な実装やインターフェースがあれば、それを使って解いてみてほしい。
（JavaならばJava Collections Frameworkが、C++ならばStandard Template Libraryがある。）

以下の問題はテキストの入力を一行ずつ読み、各行で適切なデータ構造の操作を実行することで解いてほしい。ただしファイルが百万行であっても数秒以内に処理できる程度に効率的な実装でなければならないものとする。

  \begin{enumerate}
    \item 入力を一行ずつ読み、その逆順で出力せよ。すなわち最後の入力行を最初に書き出し、最後から二番目の入力行を二番目に書き出す、というように出力せよ。

    \item  最初の50行入力を読み、それを逆順で出力せよ。その後続く50行を読み、それを逆順で出力せよ。これを読み取る行が無くなるまで繰り返し、最後に残っていた行（50行未満かもしれない）もやはり逆順で出力せよ。

      つまり、出力は50番目の行からはじまり、49、48、...、1番目の行が続く。
	  この次は100番目の行で、99、...、51番目の行が続く。

	 またプログラム実行中に50より多くの行を保持してはならない。

    \item 入力を一行ずつ読み取り、42行目以降で空行を見つけたら、その42行前の行を出力せよ。例えば、242行目が空行であれば、200行目を出力せよ。
	またプログラム実行中に43行以上の行を保持してはならない。

    \item 入力を一行ずつ読み取り、もしこれまでと重複のない行を見つけたら出力せよ。
	重複がたくさんあるファイルを読む場合にも、重複なく行を保持するのに必要なメモリより多くメモリを使わないように注意せよ。

    \item 入力を一行ずつ読み取り、それがこれまでに読んだことのある行と同じなら出力せよ。（最終的には、入力の中のはじめて現れた行を除いたものが得られる。）
	重複がたくさんあるファイルを読む場合にも、重複なく行を保持するのに必要なメモリより多くメモリを使わないように注意せよ。

    \item 入力を全て読み取り、短い順に並び替えて出力せよ。
	同じ長さの行があるときは、それらの行の順序は辞書順に並べるものとする。
	また、重複する行は一度だけ出力するものとする。

    \item 直前の問題で、重複する行は現れた回数だけ出力するように変更した問題を解け。

    \item 入力をすべて読み、全ての偶数番目の行を出力した後に全ての奇数番目の行を出力せよ。（なお、最初の行を0行目と数える。）

    \item 入力をすべて読み、ランダムに並び替えて出力せよ。
	どの行の内容も書き換えてはならない。
	また、入力とくらべて行を減らしたり増やしたりしてもいけない。
  \end{enumerate}
\end{exc}

\begin{exc}
  \index{Dyck word}%
  \emph{Dyck word}とは+1, -1からなる列で、先頭から任意の#k#番目の値までの部分列（プレフィックス）の和がいずれも非負であるものである。
  例えば、$+1,-1,+1,-1$はDyck wordだが、$+1,-1,-1,+1$は$+1-1-1<0$なのでDyck wordではない。
  Dyck wordと#Stack#の#push(x)#・#pop()#操作の関係を説明せよ。
\end{exc}

\begin{exc}
  \index{matched string}%
  \index{string!matched}%
  \emph{マッチした文字列}とは\{, \}, (, ), [, ]のからなる列で、すべての括弧が適切に対応しているものである。
  例えば、「\{\{()[]\}\}」はマッチした文字列だが、「\{\{()]\}」はふたつめの\{に対応する括弧が]であるためマッチした文字列ではない。
  長さ#n#の文字列が与えられたとき、この文字列がマッチしているかを$O(#n#)$で判定するにはスタックをどう使えばよいかを説明せよ。
\end{exc}

\begin{exc}
  #push(x)#・#pop()#操作のみが可能なスタック#s#が与えられる。
  FIFOキュー#q#だけを使って#s#の要素を逆順にする方法を説明せよ。
\end{exc}

\begin{exc}
  \index{Bag@#Bag#}%
  #USet#を使って#Bag#を実装せよ。
  #Bag#とは#USet#みたいなものである。
  #Bag#は#add(x)#・#remove(x)#・#find(x)#操作をサポートするが、重複する要素も格納するところが異なる。
  #Bag#の#find(x)#操作は#x#に等しい要素が1つ以上含まれているときそのうちのひとつを返す。
  さらに#Bag#は#findAll(x)#操作もサポートする。
  これは #Bag#に含まれる#x#に等しいすべての要素のリストを返す。
\end{exc}

\begin{exc}
  #List#・#USet#・#SSet#インターフェースのゼロから実装せよ。
  必ずしも効率的な実装でなくてもよい。
  ここで実装するものは、後の章で出てくるより効率的な実装の正しさや性能をテストするために役立つ。（最も簡単な方法は要素を配列に入れておく方法だ。）
\end{exc}

\begin{exc}
  直前の問題の実装の性能をアップするための思いつく工夫をいくつか試みよ。
  実験してみて、 #List#の#add(i,x)#・#remove(i)#の性能がどう向上したか考察せよ。
  #USet#・#SSet#の#find(x)#の性能はどうすれば向上しそうか考えてみよ。
  この問題はインターフェースの効率的な実装がどのくらい難しいかを実感するためのものである。
\end{exc}
